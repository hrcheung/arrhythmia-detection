\section{Implementation Specifics}
\subsection{Sampling strategy}
According to section 3, "Data Analysis and Preprocessing", the ECG data is divided into individual heart beats based on the annotations provided in the MIT-BIH dataset. Each heart beat is then resampled to 128 data points with respect to time and normalized to align with the x-axis. In addition, for BERT model, data is concatenated into a single long sequence of voltages for each heart beat. 

\subsection{Hyperparameter specifications}
\subsubsection{BERT}
classification layer: \\
a dropout layer with 0.1 dropout rate and a linear layer \\
loss function: Cross Entropy \\
optimizer: Adam \\
learning rate: $2^{-5}$ \\
epsilon: $1^{-8}$\\
epoch: 10 \\
batch size: 16 \\ 
shuffle data: True 

\subsubsection{Random Forest}
100 decision trees \\
The function to measure the quality of a split: Gini impurity \\
Unconstrained maximum depth of decision tree, maximum leaf nodes \\
The minimum number of samples required to split an internal node: 2 \\
The minimum number of samples required to be at a leaf node: 1 \\
The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node: 0.0

\subsection{Instruction on running code}

Open the \textbf{Detection.ipynb} with Colab. Click Run button for each section to replicate feature engineering, validating BERT Model, and Random Forest Model, except for "2.3.2 Train" in "2.3 Trained with Pre-Trained BERT".

- The feature engineering part is ready to run without loading local files. We have included wfdb API to retrieve data automatically. \\
- The "2.4 Train with Random Forest" part is ready to run on Colab. \\
- The "2.3.2 Train" part was rewritten and trained on a local machine. This part might take more than 2 hours. \\
- All the other parts in "2.3 Trained with Pre-Trained BERT" are ready to run on Colab. In addition, please change Runtime type to GPU hardware accelerator before run any code.

\subsection{Team Contribution}
In this project, we had a team consisting of three members, Haoran, Qishu, and Chenjie, who all made significant contributions towards achieving our research goals. Haoran was mainly responsible for preprocessing the data, which involved cleaning, formatting, and preparing the dataset for analysis. He utilized various techniques as mentioned in part 4.3 modeling, which helped in improving the accuracy of the models. Qishu, on the other hand, was mainly responsible for implementing BERT, a pre-trained deep learning model, for NLP tasks. She fine-tuned the model to suit the specific needs of the project and utilized it to extract relevant information from the text data. Finally, Chenjie was mainly responsible for implementing a random forest model. He fine-tuned both models and implemented various hyperparameter specifications to achieve the best possible accuracy. Together, the contributions of Haoran, Qishu, and Chenjie were instrumental in the success of the project.